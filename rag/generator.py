"""
Step 7: End-to-end RAG generator for KG-based Q&A.

Combines retrieval, prompt augmentation, and LLM generation.
Optionally expands retrieved triplets using LLM for richer context.

Supports multiple LLM providers via environment configuration:
- Local LLM: source export_local_llm.sh
- Google AI Studio: source export_google_ai.sh  
- SambaNova: source export_sambanova.sh
"""

import logging
from typing import List, Tuple, Optional, Dict, Any
from dataclasses import dataclass, field

from .kg_rag_indexer import KGRagIndexer
from .retriever import KGRetriever, RetrievalResult, ContextFormat
from .prompt_builder import KGPromptBuilder, get_prompt_builder
from .triplet_expander import TripletExpander, get_triplet_expander

# Import the unified chat completion function that routes to local or API
from .edc.edc.utils.llm_utils import openai_chat_completion

logger = logging.getLogger(__name__)


@dataclass
class GenerationResult:
    """
    Result from KG-RAG generation.
    
    Attributes:
        answer: Generated answer text
        sources: List of (subject, predicate, object) triplets used
        scores: Relevance scores of source triplets
        query: Original query
        context: Full retrieval result
        prompt: The augmented prompt sent to the LLM
        expanded_triplets: Triplets generated by LLM expansion (if enabled)
        persisted_count: Number of expanded triplets persisted to index (if persist_expanded=True)
    """
    answer: str
    sources: List[Tuple[str, str, str]]
    scores: List[float]
    query: str
    context: Optional[RetrievalResult] = None
    prompt: Optional[str] = None
    expanded_triplets: Optional[List[Tuple[str, str, str]]] = None
    persisted_count: int = 0
    
    def __repr__(self) -> str:
        expanded_info = f", expanded={len(self.expanded_triplets)}" if self.expanded_triplets else ""
        return f"GenerationResult(answer='{self.answer[:50]}...', sources={len(self.sources)}{expanded_info})"
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        result = {
            "answer": self.answer,
            "query": self.query,
            "sources": [
                {"subject": s, "predicate": p, "object": o, "score": score}
                for (s, p, o), score in zip(self.sources, self.scores)
            ],
            "num_sources": len(self.sources),
        }
        if self.expanded_triplets:
            result["expanded_triplets"] = [
                {"subject": s, "predicate": p, "object": o}
                for s, p, o in self.expanded_triplets
            ]
            result["num_expanded"] = len(self.expanded_triplets)
        if self.persisted_count > 0:
            result["persisted_count"] = self.persisted_count
        return result


class KGRagGenerator:
    """
    End-to-end RAG pipeline for KG-based question answering.
    
    Combines:
    1. KGRetriever - Retrieve relevant triplets
    2. TripletExpander (optional) - Expand triplets using LLM
    3. KGPromptBuilder - Build augmented prompt
    4. LLM - Generate answer (via openai_chat_completion)
    
    Supports multiple LLM providers based on environment configuration:
    - Local LLM: source export_local_llm.sh (requires GPU + bitsandbytes)
    - Google AI Studio: source export_google_ai.sh
    - SambaNova: source export_sambanova.sh
    
    Usage:
        # Configure provider first: source export_google_ai.sh
        indexer = KGRagIndexer.load("./output/rag")
        generator = KGRagGenerator(indexer)
        
        # Generate with triplet expansion (default)
        result = generator.generate("Where is Trane located?", expand_triplets=True)
        print(result.answer)
        print(f"Expanded triplets: {result.expanded_triplets}")
    """
    
    def __init__(
        self,
        indexer: KGRagIndexer,
        template_path: Optional[str] = None,
        context_format: ContextFormat = ContextFormat.TRIPLET_LIST,
        schema_path: Optional[str] = None,
        expansion_template_path: Optional[str] = None,
        index_path: Optional[str] = None,
    ):
        """
        Initialize the generator.
        
        Args:
            indexer: A loaded KGRagIndexer instance
            template_path: Optional custom prompt template path for QA
            context_format: Format for retrieved context
            schema_path: Optional path to schema CSV for triplet expansion
            expansion_template_path: Optional custom template for triplet expansion
            index_path: Path to the index directory (for auto-save functionality)
        """
        self.indexer = indexer
        self.retriever = KGRetriever(indexer, default_format=context_format)
        self.prompt_builder = get_prompt_builder(template_path=template_path)
        self.expander = get_triplet_expander(
            schema_path=schema_path,
            template_path=expansion_template_path,
        )
        self._index_path = index_path  # Store for auto-save
    
    def generate(
        self,
        query: str,
        top_k: int = 10,
        temperature: float = 0.1,
        max_tokens: int = 256,
        include_scores: bool = False,
        return_prompt: bool = False,
        expand_triplets: bool = False,
        max_expansion: int = 10,
        expansion_temperature: float = 0.3,
        persist_expanded: bool = False,
        auto_save: bool = False,
        similarity_threshold: float = 0.95,
    ) -> GenerationResult:
        """
        Generate an answer using the full RAG pipeline.
        
        Args:
            query: Natural language question
            top_k: Number of triplets to retrieve
            temperature: LLM sampling temperature
            max_tokens: Maximum tokens to generate
            include_scores: Include relevance scores in prompt
            return_prompt: Include the prompt in the result
            expand_triplets: Whether to expand retrieved triplets using LLM
            max_expansion: Maximum number of triplets to generate during expansion
            expansion_temperature: LLM temperature for triplet expansion
            persist_expanded: Whether to persist expanded triplets to the index
            auto_save: Whether to auto-save the index after persisting triplets
            similarity_threshold: Cosine similarity threshold for duplicate detection
            
        Returns:
            GenerationResult with answer and sources
        """
        logger.info(f"Generating answer for: {query}")
        
        # Step 5: Retrieve relevant triplets
        logger.debug("Step 5: Retrieving relevant triplets...")
        context = self.retriever.retrieve(query, top_k=top_k)
        logger.debug(f"Retrieved {len(context)} triplets")
        
        # Step 5.5: Expand triplets using LLM (optional)
        expanded_triplets = None
        all_triplets = context.triplets
        all_scores = context.scores
        persisted_count = 0
        
        if expand_triplets and context.triplets:
            logger.debug("Step 5.5: Expanding triplets with LLM...")
            expanded_triplets = self.expander.expand(
                query=query,
                retrieved_triplets=context.triplets,
                max_new_triplets=max_expansion,
                temperature=expansion_temperature,
            )
            logger.debug(f"Generated {len(expanded_triplets)} expanded triplets")
            
            # Combine original and expanded triplets
            if expanded_triplets:
                all_triplets = list(context.triplets) + expanded_triplets
                # Assign lower scores to expanded triplets (they're LLM-generated)
                all_scores = list(context.scores) + [0.5] * len(expanded_triplets)
                
                # Create an updated context with combined triplets
                context = self._create_expanded_context(
                    context, expanded_triplets, all_triplets, all_scores
                )
        
        # Step 6: Build augmented prompt
        logger.debug("Step 6: Building augmented prompt...")
        messages = self.prompt_builder.build_chat_messages(
            query, context, include_scores=include_scores
        )
        system_prompt = self.prompt_builder.get_system_prompt()
        
        # Step 7: Generate with LLM (routes to local or API based on env config)
        logger.debug("Step 7: Generating with LLM...")
        answer = openai_chat_completion(
            system_prompt=system_prompt,
            history=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        
        logger.info(f"Generated answer: {answer[:100]}...")
        
        # Step 7.5: Persist expanded triplets to index (optional, offline learning)
        if persist_expanded and expanded_triplets:
            logger.debug("Step 7.5: Persisting expanded triplets to index...")
            persisted_count = self.indexer.add_expanded_triplets(
                triplets=expanded_triplets,
                check_duplicates=True,
                similarity_threshold=similarity_threshold,
            )
            
            if persisted_count > 0:
                logger.info(f"Persisted {persisted_count} new triplets to index")
                
                # Auto-save if enabled and we have a path
                if auto_save and self._index_path:
                    logger.info(f"Auto-saving index to {self._index_path}...")
                    self.indexer.save(self._index_path)
                elif auto_save and not self._index_path:
                    logger.warning("auto_save=True but no index_path configured. Call save() manually.")
        
        # Build result
        result = GenerationResult(
            answer=answer,
            sources=all_triplets,
            scores=all_scores,
            query=query,
            context=context if return_prompt else None,
            prompt=self.prompt_builder.build(query, context) if return_prompt else None,
            expanded_triplets=expanded_triplets,
            persisted_count=persisted_count,
        )
        
        return result
    
    def _create_expanded_context(
        self,
        original_context: RetrievalResult,
        expanded_triplets: List[Tuple[str, str, str]],
        all_triplets: List[Tuple[str, str, str]],
        all_scores: List[float],
    ) -> RetrievalResult:
        """
        Create an updated RetrievalResult with expanded triplets included.
        
        Args:
            original_context: Original retrieval result
            expanded_triplets: Triplets generated by expansion
            all_triplets: Combined list of all triplets
            all_scores: Combined list of all scores
            
        Returns:
            New RetrievalResult with expanded context
        """
        # Format expanded triplets for context
        expanded_lines = []
        for s, p, o in expanded_triplets:
            s_h = s.replace("_", " ")
            p_h = p.replace("_", " ")
            o_h = o.replace("_", " ")
            expanded_lines.append(f"({s_h}, {p_h}, {o_h})")
        
        # Append to formatted context
        expanded_context_str = "\n".join(expanded_lines)
        new_formatted = original_context.formatted_context
        if expanded_lines:
            new_formatted += "\n--- LLM-Expanded Facts ---\n" + expanded_context_str
        
        return RetrievalResult(
            triplets=all_triplets,
            formatted_context=new_formatted,
            scores=all_scores,
            raw_results=original_context.raw_results,
        )
    
    def generate_batch(
        self,
        queries: List[str],
        top_k: int = 10,
        temperature: float = 0.1,
        max_tokens: int = 256,
        expand_triplets: bool = False,
        max_expansion: int = 10,
        persist_expanded: bool = False,
        auto_save: bool = False,
        similarity_threshold: float = 0.95,
    ) -> List[GenerationResult]:
        """
        Generate answers for multiple queries.
        
        Args:
            queries: List of questions
            top_k: Number of triplets per query
            temperature: LLM sampling temperature
            max_tokens: Maximum tokens per answer
            expand_triplets: Whether to expand retrieved triplets using LLM
            max_expansion: Maximum number of triplets to generate during expansion
            persist_expanded: Whether to persist expanded triplets to the index
            auto_save: Whether to auto-save the index after persisting triplets
            similarity_threshold: Cosine similarity threshold for duplicate detection
            
        Returns:
            List of GenerationResult objects
        """
        results = []
        for query in queries:
            result = self.generate(
                query,
                top_k=top_k,
                temperature=temperature,
                max_tokens=max_tokens,
                expand_triplets=expand_triplets,
                max_expansion=max_expansion,
                persist_expanded=persist_expanded,
                auto_save=False,  # Don't auto-save on each query in batch
                similarity_threshold=similarity_threshold,
            )
            results.append(result)
        
        # Save once at the end if auto_save is enabled
        if auto_save and persist_expanded and self._index_path:
            total_persisted = sum(r.persisted_count for r in results)
            if total_persisted > 0:
                logger.info(f"Auto-saving index after batch (persisted {total_persisted} total triplets)...")
                self.indexer.save(self._index_path)
        
        return results
    
    def retrieve_only(
        self,
        query: str,
        top_k: int = 10,
    ) -> RetrievalResult:
        """
        Only retrieve triplets without LLM generation.
        
        Useful for debugging or when you want to handle generation separately.
        
        Args:
            query: Natural language question
            top_k: Number of triplets to retrieve
            
        Returns:
            RetrievalResult with triplets and formatted context
        """
        return self.retriever.retrieve(query, top_k=top_k)
    
    def get_prompt(
        self,
        query: str,
        top_k: int = 10,
        include_scores: bool = False,
    ) -> str:
        """
        Get the prompt that would be sent to the LLM.
        
        Useful for debugging or customization.
        
        Args:
            query: Natural language question
            top_k: Number of triplets to retrieve
            include_scores: Include relevance scores
            
        Returns:
            Formatted prompt string
        """
        context = self.retriever.retrieve(query, top_k=top_k)
        return self.prompt_builder.build(query, context, include_scores=include_scores)
    
    def save(self, path: Optional[str] = None) -> None:
        """
        Save the index to disk.
        
        Useful for saving after persist_expanded operations.
        
        Args:
            path: Directory to save to (uses stored _index_path if not provided)
        """
        save_path = path or self._index_path
        if not save_path:
            raise ValueError("No save path provided and no index_path configured")
        
        self.indexer.save(save_path)
        logger.info(f"Saved index to {save_path}")
    
    def set_index_path(self, path: str) -> None:
        """
        Set the index path for auto-save functionality.
        
        Args:
            path: Path to the index directory
        """
        self._index_path = path


def create_generator(
    index_path: str,
    template_path: Optional[str] = None,
    device: Optional[str] = None,
    schema_path: Optional[str] = None,
    expansion_template_path: Optional[str] = None,
) -> KGRagGenerator:
    """
    Convenience function to create a KGRagGenerator.
    
    LLM provider is determined by environment configuration:
    - source export_local_llm.sh for local LLM
    - source export_google_ai.sh for Google AI Studio
    - source export_sambanova.sh for SambaNova
    
    Args:
        index_path: Path to saved FAISS index
        template_path: Optional prompt template path for QA
        device: Device for embeddings
        schema_path: Optional path to schema CSV for triplet expansion
        expansion_template_path: Optional custom template for triplet expansion
        
    Returns:
        Configured KGRagGenerator
    """
    indexer = KGRagIndexer.load(index_path, device=device)
    return KGRagGenerator(
        indexer=indexer,
        template_path=template_path,
        schema_path=schema_path,
        expansion_template_path=expansion_template_path,
        index_path=index_path,  # Store path for auto-save functionality
    )


if __name__ == "__main__":
    import argparse
    
    logging.basicConfig(level=logging.INFO)
    
    parser = argparse.ArgumentParser(description="Test KG-RAG Generator")
    parser.add_argument("--index", required=True, help="Path to FAISS index")
    parser.add_argument("--query", required=True, help="Question to answer")
    parser.add_argument("--top_k", type=int, default=5, help="Number of triplets to retrieve")
    parser.add_argument("--expand", action="store_true", help="Enable triplet expansion")
    parser.add_argument("--max_expansion", type=int, default=10, help="Max triplets to expand")
    parser.add_argument("--schema", default=None, help="Path to schema CSV for expansion")
    
    args = parser.parse_args()
    
    # Create generator
    generator = create_generator(args.index, schema_path=args.schema)
    
    # Generate
    result = generator.generate(
        args.query,
        top_k=args.top_k,
        return_prompt=True,
        expand_triplets=args.expand,
        max_expansion=args.max_expansion,
    )
    
    print("\n" + "=" * 60)
    print(f"Query: {result.query}")
    print("=" * 60)
    
    print("\n=== Retrieved Sources ===")
    for (s, p, o), score in zip(result.sources, result.scores):
        print(f"  [{score:.3f}] ({s}, {p}, {o})")
    
    if result.expanded_triplets:
        print(f"\n=== Expanded Triplets ({len(result.expanded_triplets)}) ===")
        for s, p, o in result.expanded_triplets:
            print(f"  ({s}, {p}, {o})")
    
    print("\n=== Generated Answer ===")
    print(result.answer)
    print("=" * 60)


